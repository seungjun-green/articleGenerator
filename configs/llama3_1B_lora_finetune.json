{
    "base_model_name": "meta-llama/Llama-3.2-1B",
    "lora_rank": 16,
    "lora_alpha": 32,
    "learning_rate": 5e-5,
    "batch_size": 32,
    "num_epochs": 10,
    "output_dir": "./fine_tuned_checkpoints",
    "save_steps": 200,
    "eval_steps": 100,
    "max_seq_length": 256
}